{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Models\n",
    "\n",
    "In the previous notebook, we saw that the performance of the simple neural network was very poor even after adding many hidden nodes. We can add more hidden layers to that network but this would result in the neural network taking a lot of time to fit to the training data.\n",
    "\n",
    "Convolutional neural networks performance best for image and character recognition problems because if we consider any image, proximity has a strong relation with similarity in it and convolutional neural networks specifically take advantage of this fact. This implies, in a given image, two pixels that are nearer to each other are more likely to be related than the two pixels that are apart from each other. Nevertheless, in a usual neural network, every pixel is linked to every single neuron. The added computational load makes the simple neural network less accurate in this case. By killing a lot of these less significant connections, convolution solves this problem. In technical terms, convolutional neural networks make the image processing computationally manageable through filtering the connections by proximity.\n",
    "\n",
    "## Loading Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n",
      "0   45    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "1   36    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "2   43    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "3   15    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "4    4    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0  \n",
      "3    0    0    0    0    0    0  \n",
      "4    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n",
      "0   41    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "1   39    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "2    9    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "3   26    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "4   44    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0  \n",
      "3    0    0    0    0    0    0  \n",
      "4    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(112800, 785)\n",
      "(18800, 785)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/emnist-balanced-train.csv\", header = None)\n",
    "test = pd.read_csv(\"../data/emnist-balanced-test.csv\", header = None)\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data in the correct format to be able to feed to the convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate out the train data from the response variable\n",
    "X_train = train.iloc[:, 1:]\n",
    "X_test = test.iloc[:, 1:]\n",
    "\n",
    "# separate out the response variable from the data\n",
    "Y_train = train[0]\n",
    "Y_test = test[0]\n",
    "\n",
    "# converting the pandas dataframe to numpy matrices\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# reshaping the data into the format which can be passed to the neural network\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# converting the data type to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another change that we make in this notebook from the previous one is to actually normalize all the predictors so that they takes values from [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the predictors\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "\n",
    "# Defining the number of classes in the response variable\n",
    "number_of_classes = 47\n",
    "\n",
    "# One hot encoding the response variable\n",
    "Y_train = to_categorical(Y_train, number_of_classes)\n",
    "Y_test = to_categorical(Y_test, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ConvNet\n",
    "\n",
    "Building a sparse convolutional neural network model by following these methods:\n",
    "\n",
    "1. Add convolution layers\n",
    "2. Add activation function\n",
    "3. Add pooling layers\n",
    "4. Repeat Steps 1,2,3 for adding more hidden layers\n",
    "5. Finally, add a fully connected softmax layer giving the CNN the ability to classify the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the number of classes in the response variable\n",
    "number_of_classes = 47\n",
    "\n",
    "# Adding the first set of convolutional and pooling layers with ReLu activation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Adding the second set of convolutional and pooling layers with ReLu activation\n",
    "model.add(Conv2D(64,(3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Adding fully connected layers with softmax activation and 20% dropout\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(number_of_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compiling the model with categorical crossentropy loss function to handle multiple classes\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the ConvNet\n",
    "\n",
    "Fit the convolutional neural network model on the training data and evaluate on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112800 samples, validate on 18800 samples\n",
      "Epoch 1/10\n",
      "112800/112800 [==============================] - 1465s 13ms/step - loss: 0.5719 - acc: 0.8188 - val_loss: 0.4284 - val_acc: 0.8554\n",
      "Epoch 2/10\n",
      "112800/112800 [==============================] - 1636s 14ms/step - loss: 0.3399 - acc: 0.8777 - val_loss: 0.3702 - val_acc: 0.8674\n",
      "Epoch 3/10\n",
      "112800/112800 [==============================] - 8618s 76ms/step - loss: 0.2975 - acc: 0.8900 - val_loss: 0.3885 - val_acc: 0.8675\n",
      "Epoch 4/10\n",
      "112800/112800 [==============================] - 2099s 19ms/step - loss: 0.2733 - acc: 0.8974 - val_loss: 0.3394 - val_acc: 0.8790\n",
      "Epoch 5/10\n",
      "112800/112800 [==============================] - 1365s 12ms/step - loss: 0.2512 - acc: 0.9035 - val_loss: 0.3330 - val_acc: 0.8835\n",
      "Epoch 6/10\n",
      "112800/112800 [==============================] - 1392s 12ms/step - loss: 0.2365 - acc: 0.9089 - val_loss: 0.3323 - val_acc: 0.8852\n",
      "Epoch 7/10\n",
      "112800/112800 [==============================] - 1477s 13ms/step - loss: 0.2213 - acc: 0.9134 - val_loss: 0.3265 - val_acc: 0.8876\n",
      "Epoch 8/10\n",
      "112800/112800 [==============================] - 1470s 13ms/step - loss: 0.2056 - acc: 0.9187 - val_loss: 0.3409 - val_acc: 0.8821\n",
      "Epoch 9/10\n",
      "112800/112800 [==============================] - 1487s 13ms/step - loss: 0.1934 - acc: 0.9227 - val_loss: 0.3354 - val_acc: 0.8873\n",
      "Epoch 10/10\n",
      "112800/112800 [==============================] - 1545s 14ms/step - loss: 0.1803 - acc: 0.9272 - val_loss: 0.3436 - val_acc: 0.8839\n",
      "18800/18800 [==============================] - 92s 5ms/step\n",
      "Test score: 0.34364881123634095\n",
      "Test accuracy: 0.8838829787234043\n"
     ]
    }
   ],
   "source": [
    "# Setting the batch size and number of epochs\n",
    "batch_size=256\n",
    "epochs=10\n",
    "\n",
    "# Training the model on the train data\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, the performance of the convolutional neural network as a lot better than that of the simple neural nets we built in the previous notebook. This is beacuse of the reasons we discussed at the beginning of this notebook. Now, that we are done with the exploratory part of the model building process, we will actually start building more complex convolutional neural network models on the balanced data and also on the byclass data. The code for these models can be found in the directory [develop/src/models](../src/models).\n",
    "\n",
    "We tried the following different models.\n",
    "\n",
    "1. Sparse convolutional neural network on balanced data (exactly the one trained above)\n",
    "2. Dense convolutional neural network on the balanced data which has 64 filters in the first two convolutional layers (instead of 32 here) and 128 filters in the last two convolutional layers (instead of 64 here) and finally 1024 hidden nodes in the last fully connected layer.\n",
    "3. Sparse convolutional neural network on the byclass data (same as the one described in 1 but for the byclass data).\n",
    "4. Dense covolutional neural network on the byclass data (same as the one described in 2 but for the byclass data)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
